{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ef3c41",
   "metadata": {},
   "source": [
    "# Automated Data Observability Pipeline for Tableau Cloud\n",
    "\n",
    "### Project Overview\n",
    "In large-scale Analytics environments, relying on users to report stale data is inefficient and damages trust. This project shifts the paradigm from **Reactive Support** to **Proactive Observability**.\n",
    "\n",
    "Using the **Tableau Server Client (TSC) API**, this pipeline:\n",
    "1.  **Audits** the entire Tableau Cloud environment automatically.\n",
    "2.  **Validates** the \"Data Freshness\" of extracts against defined SLAs.\n",
    "3.  **Identifies** failures before they impact business decision-making.\n",
    "\n",
    "> This is a **blueprint / template**, not a production-ready pipeline.\n",
    "Some sections intentionally use pseudocode to focus on architecture and reasoning.\n",
    "\n",
    "### Tech Stack\n",
    "* **Python 3.x**\n",
    "* **Tableau Server Client (TSC)** for API interaction\n",
    "* **Pandas** for structured log analysis\n",
    "* **Data Governance** concepts (SLA monitoring)\n",
    "\n",
    "### Data Freshness Logic (Conceptual)\n",
    "\n",
    "This notebook uses a simplified and conceptual approach to evaluate data freshness.\n",
    "\n",
    "Important:\n",
    "- `last_refresh` is treated as a placeholder for extract refresh metadata.\n",
    "- Assets without applicable refresh information (e.g. live connections) are classified as `NotApplicable`.\n",
    "- `Unknown` or missing metadata is never treated as `Healthy`.\n",
    "\n",
    "This approach avoids false-positive health indicators and reflects observability best practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4baed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Tableau Server Client library\n",
    "# This is required as it is not included in the standard Databricks Runtime.\n",
    "\n",
    "%pip install tableauserverclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75443dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tableauserverclient as TSC\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timezone\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ebcc1",
   "metadata": {},
   "source": [
    "## Authentication Configuration\n",
    "\n",
    "Authentication is handled via Personal Access Tokens (PAT).\n",
    "In Databricks, credentials should be retrieved securely using Secret Scopes.\n",
    "\n",
    "This notebook assumes the following secrets exist:\n",
    "- tableau-pat-name\n",
    "- tableau-pat-value\n",
    "- tableau-site-id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e264ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "SERVER_URL = \"https://prod-useast-b.online.tableau.com\"\n",
    "\n",
    "try:\n",
    "    TOKEN_NAME  = dbutils.secrets.get(scope=\"tableau\", key=\"pat-name\")\n",
    "    TOKEN_VALUE = dbutils.secrets.get(scope=\"tableau\", key=\"pat-value\")\n",
    "    SITE_ID     = dbutils.secrets.get(scope=\"tableau\", key=\"site-id\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Tableau credentials not found. \"\n",
    "        \"This notebook expects PAT credentials stored in Databricks Secrets.\"\n",
    "    )\n",
    "\n",
    "tableau_auth = TSC.PersonalAccessTokenAuth(\n",
    "    token_name=TOKEN_NAME,\n",
    "    personal_access_token=TOKEN_VALUE,\n",
    "    site_id=SITE_ID\n",
    ")\n",
    "\n",
    "server = TSC.Server(SERVER_URL, use_server_version=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0514e",
   "metadata": {},
   "source": [
    "## Data Freshness Evaluation Logic (Conceptual)\n",
    "\n",
    "Observability distinguishes between **absence of data** and **healthy data**.\n",
    "\n",
    "Classification rules:\n",
    "- Healthy: Extract refreshed within SLA\n",
    "- Critical: Extract refresh exceeds SLA\n",
    "- NotApplicable: Live connections or assets without refresh semantics\n",
    "- Unknown: Metadata exists but freshness cannot be determined\n",
    "\n",
    "Important:\n",
    "- Missing refresh timestamps are never treated as \"Healthy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da873680",
   "metadata": {},
   "outputs": [],
   "source": [
    "now_utc = datetime.datetime.now(timezone.utc)\n",
    "\n",
    "def calculate_freshness(last_refresh, sla_hours=24):\n",
    "    \"\"\"\n",
    "    Conceptual freshness evaluation.\n",
    "    \n",
    "    Parameters:\n",
    "    - last_refresh: datetime or None\n",
    "    - sla_hours: SLA threshold\n",
    "    \n",
    "    Returns:\n",
    "    - hours_since_refresh (float or None)\n",
    "    - status (str)\n",
    "    \"\"\"\n",
    "    if last_refresh is None:\n",
    "        return None, 'NotApplicable'\n",
    "\n",
    "    hours_since = (now_utc - last_refresh).total_seconds() / 3600\n",
    "\n",
    "    if hours_since > sla_hours:\n",
    "        return round(hours_since, 2), 'Critical'\n",
    "\n",
    "    return round(hours_since, 2), 'Healthy'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b2e12",
   "metadata": {},
   "source": [
    "## Metadata Extraction Pipeline\n",
    "\n",
    "This section iterates through Tableau Cloud assets and builds\n",
    "a unified audit log for observability analysis.\n",
    "\n",
    "⚠️ API calls are simplified and serve as conceptual placeholders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_log = []\n",
    "\n",
    "print(f\"Starting Audit on: {SERVER_URL}\")\n",
    "\n",
    "with server.auth.sign_in(tableau_auth):\n",
    "\n",
    "    # --- WORKBOOK AUDIT (PSEUDOCODE) ---\n",
    "    print(\"Scanning Workbooks...\")\n",
    "    all_workbooks, _ = server.workbooks.get(TSC.RequestOptions())\n",
    "\n",
    "    for wb in all_workbooks:\n",
    "        # Conceptual placeholder: real implementations may use extract refresh endpoints\n",
    "        last_refresh = wb.updated_at\n",
    "\n",
    "        hours_since, status = calculate_freshness(last_refresh)\n",
    "\n",
    "        audit_log.append({\n",
    "            'asset_type': 'Workbook',\n",
    "            'asset_id': wb.id,\n",
    "            'asset_name': wb.name,\n",
    "            'owner_id': wb.owner_id,\n",
    "            'project_name': wb.project_name,\n",
    "            'last_refresh_utc': last_refresh,\n",
    "            'hours_since_refresh': hours_since,\n",
    "            'status': status,\n",
    "            'audit_timestamp': now_utc\n",
    "        })\n",
    "\n",
    "    # --- DATASOURCE AUDIT (PSEUDOCODE) ---\n",
    "    print(\"Scanning Datasources...\")\n",
    "    all_datasources, _ = server.datasources.get(TSC.RequestOptions())\n",
    "\n",
    "    for ds in all_datasources:\n",
    "        last_refresh = ds.updated_at  # Conceptual placeholder\n",
    "\n",
    "        hours_since, status = calculate_freshness(last_refresh)\n",
    "\n",
    "        audit_log.append({\n",
    "            'asset_type': 'Datasource',\n",
    "            'asset_id': ds.id,\n",
    "            'asset_name': ds.name,\n",
    "            'owner_id': ds.owner_id,\n",
    "            'project_name': ds.project_name,\n",
    "            'last_refresh_utc': last_refresh,\n",
    "            'hours_since_refresh': hours_since,\n",
    "            'status': status,\n",
    "            'audit_timestamp': now_utc\n",
    "        })\n",
    "\n",
    "print(f\"Audit Complete. Extracted {len(audit_log)} records.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b3631e",
   "metadata": {},
   "source": [
    "## Persistence (Bronze Layer)\n",
    "\n",
    "The audit log is persisted as a Delta table.\n",
    "This layer is intended for downstream analytics, alerting, and dashboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5993a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_df = spark.createDataFrame(audit_log)\n",
    "\n",
    "(\n",
    "    audit_df\n",
    "    .write\n",
    "    .mode(\"append\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(\"tableau_observability_bronze\")\n",
    ")\n",
    "\n",
    "display(audit_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd453963",
   "metadata": {},
   "source": [
    "## Expected Outputs\n",
    "\n",
    "- Unified audit table containing:\n",
    "  - Asset metadata\n",
    "  - Data freshness indicators\n",
    "  - SLA-based health classification\n",
    "- Bronze layer ready for:\n",
    "  - Alerting\n",
    "  - Governance dashboards\n",
    "  - Historical trend analysis\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Integrate Tableau Metadata API / GraphQL for lineage and field-level observability\n",
    "- Introduce project-specific SLAs\n",
    "- Build alerting on Critical assets\n",
    "- Create a Gold-layer dashboard for operational monitoring\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
