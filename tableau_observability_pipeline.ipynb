{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45ef3c41",
   "metadata": {},
   "source": [
    "# Automated Data Observability Pipeline for Tableau Cloud\n",
    "\n",
    "### Project Overview\n",
    "In large-scale Analytics environments, relying on users to report stale data is inefficient and damages trust. This project shifts the paradigm from **Reactive Support** to **Proactive Observability**.\n",
    "\n",
    "Using the **Tableau Server Client (TSC) API**, this pipeline:\n",
    "1.  **Audits** the entire Tableau Cloud environment automatically.\n",
    "2.  **Validates** the \"Data Freshness\" of extracts against defined SLAs.\n",
    "3.  **Identifies** failures before they impact business decision-making.\n",
    "\n",
    "### Tech Stack\n",
    "* **Python 3.x**\n",
    "* **Tableau Server Client (TSC)** for API interaction\n",
    "* **Pandas** for structured log analysis\n",
    "* **Data Governance** concepts (SLA monitoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4baed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Tableau Server Client library\n",
    "# This is required as it is not included in the standard Databricks Runtime.\n",
    "\n",
    "%pip install tableauserverclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7021ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tableauserverclient as TSC\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timezone\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# --- SECURITY CONFIGURATION ---\n",
    "# Best Practice: Retrieve credentials from Databricks Secrets (Key Vault)\n",
    "# instead of hardcoding sensitive information in the notebook.\n",
    "try:\n",
    "    # Replace 'tableau_secrets' with your actual Secret Scope name\n",
    "    TOKEN_NAME = dbutils.secrets.get(scope=\"tableau_secrets\", key=\"pat_name\")\n",
    "    TOKEN_VALUE = dbutils.secrets.get(scope=\"tableau_secrets\", key=\"pat_value\")\n",
    "    \n",
    "    # Environment Configuration\n",
    "    SITE_ID = \"corporate-analytics\" # Replace with your Site ID\n",
    "    SERVER_URL = \"https://prod-useast-b.online.tableau.com\" # Replace with your Server URL\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not retrieve secrets from Databricks. Error: {e}\")\n",
    "    print(\"Ensure the Secret Scope is mounted and keys are correct.\")\n",
    "\n",
    "# Initialize Authentication\n",
    "tableau_auth = TSC.PersonalAccessTokenAuth(TOKEN_NAME, TOKEN_VALUE, SITE_ID)\n",
    "server = TSC.Server(SERVER_URL, use_server_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd7ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DATA EXTRACTION PIPELINE ---\n",
    "# Goals:\n",
    "# 1. Connect to Tableau Cloud API\n",
    "# 2. Iterate through all Workbooks and Published Datasources\n",
    "# 3. Calculate 'Data Freshness' (Lag)\n",
    "# 4. Generate a unified audit log\n",
    "\n",
    "audit_log = []\n",
    "\n",
    "print(f\"Starting Audit on: {SERVER_URL}\")\n",
    "\n",
    "with server.auth.sign_in(tableau_auth):\n",
    "    \n",
    "    # 1. Audit Workbooks\n",
    "    print(\"Scanning Workbooks...\")\n",
    "    all_workbooks, _ = server.workbooks.get(TSC.RequestOptions())\n",
    "    \n",
    "    for wb in all_workbooks:\n",
    "        last_refresh = wb.updated_at\n",
    "        # Calculate hours since last refresh. If None (Live connection), set to -1\n",
    "        hours_since = (datetime.datetime.now(timezone.utc) - last_refresh).total_seconds() / 3600 if last_refresh else -1\n",
    "        \n",
    "        audit_log.append({\n",
    "            'asset_type': 'Workbook',\n",
    "            'asset_id': wb.id,\n",
    "            'asset_name': wb.name,\n",
    "            'owner_id': wb.owner_id,\n",
    "            'project_name': wb.project_name,\n",
    "            'last_refresh_utc': last_refresh,\n",
    "            'hours_since_refresh': float(round(hours_since, 2)),\n",
    "            'status': 'Critical' if hours_since > 24 else 'Healthy', # SLA Rule: 24h\n",
    "            'audit_timestamp': datetime.datetime.now(timezone.utc)\n",
    "        })\n",
    "\n",
    "    # 2. Audit Published Datasources\n",
    "    print(\"Scanning Datasources...\")\n",
    "    all_datasources, _ = server.datasources.get(TSC.RequestOptions())\n",
    "    \n",
    "    for ds in all_datasources:\n",
    "        last_refresh = ds.updated_at\n",
    "        hours_since = (datetime.datetime.now(timezone.utc) - last_refresh).total_seconds() / 3600 if last_refresh else -1\n",
    "        \n",
    "        audit_log.append({\n",
    "            'asset_type': 'Datasource',\n",
    "            'asset_id': ds.id,\n",
    "            'asset_name': ds.name,\n",
    "            'owner_id': ds.owner_id,\n",
    "            'project_name': ds.project_name,\n",
    "            'last_refresh_utc': last_refresh,\n",
    "            'hours_since_refresh': float(round(hours_since, 2)),\n",
    "            'status': 'Critical' if hours_since > 24 else 'Healthy', # SLA Rule: 24h\n",
    "            'audit_timestamp': datetime.datetime.now(timezone.utc)\n",
    "        })\n",
    "\n",
    "print(f\"Audit Complete. Extracted {len(audit_log)} metadata records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- STORAGE LAYER (Load) ---\n",
    "# Convert the audit log to a Spark DataFrame and save it as a Delta Table.\n",
    "# This enables historical analysis of environment stability over time.\n",
    "\n",
    "# 1. Convert List of Dicts -> Pandas -> Spark\n",
    "# (Pandas is used as an intermediate step for easier handling of list of dicts)\n",
    "df_pandas = pd.DataFrame(audit_log)\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# 2. Define Table Location\n",
    "table_name = \"bronze_tableau_observability_logs\"\n",
    "\n",
    "# 3. Write to Delta Lake\n",
    "(df_spark.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\") # Append ensures we keep history\n",
    "    .option(\"mergeSchema\", \"true\") # Handles schema evolution if we add fields later\n",
    "    .saveAsTable(table_name)\n",
    ")\n",
    "\n",
    "print(f\"Success! Data written to Delta Table: '{table_name}'\")\n",
    "display(df_spark.limit(5))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
